{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiVgB1ujJgGU",
        "outputId": "c8d3bf83-9aea-4869-f412-920baf528306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!', ' ']\n",
            "size is: 31\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "char_to_num = tf.keras.layers.StringLookup(vocabulary=characters,oov_token=\"\")\n",
        "num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(),oov_token=\"\",invert=True)\n",
        "\n",
        "\n",
        "print(f\"Vocabulary is: {char_to_num.get_vocabulary()}\")\n",
        "print(f\"size is: {char_to_num.vocabulary_size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame_length = 256\n",
        "frame_step = 160\n",
        "fft_length = 384"
      ],
      "metadata": {
        "id": "ZWceLuTVV7uc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the audio file and squeeze it to mono\n",
        "audio, sr = librosa.load('audio_file.wav', sr=16000, mono=True)"
      ],
      "metadata": {
        "id": "JpAZkbNHjUkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import librosa\n",
        "\n",
        "\n",
        "\n",
        "# Define the function to preprocess the audio\n",
        "def preprocess_audio(audio):\n",
        "\n",
        "  # Convert the audio to float32 and reshape to a 2D tensor\n",
        "  audio = tf.reshape(tf.cast(audio, tf.float32), [1, -1])\n",
        "\n",
        "  # Perform STFT with absolute value and raise to the power of 0.5\n",
        "  stft = tf.abs(tf.signal.stft(audio, frame_length=512, frame_step=160, fft_length=512))\n",
        "\n",
        "  # Raise the spectrogram to the power of 0.5\n",
        "  spectrogram = tf.pow(stft, 0.5)\n",
        "\n",
        "  # Compute the mean and standard deviation of the spectrogram\n",
        "  mean = tf.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "  stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "\n",
        "  # Normalize the spectrogram by subtracting the mean and dividing by the standard deviation\n",
        "  spectrogram = (spectrogram - mean) / (stddevs + 1e-10)\n",
        "\n",
        "  # Convert the label to lowercase\n",
        "  label = tf.strings.lower(label)\n",
        "\n",
        "  # Split the label into a list of Unicode characters\n",
        "  label_chars = tf.strings.unicode_split(label, 'UTF-8')\n",
        "\n",
        "  label = char_to_num(label)\n",
        "  return spectrogram"
      ],
      "metadata": {
        "id": "VHelkkC7emjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to preprocess the labels\n",
        "def preprocess_label(label):\n",
        "    # Convert the label to lowercase and split into Unicode characters\n",
        "    label = tf.strings.lower(label)\n",
        "    label = tf.strings.unicode_split(label, 'UTF-8')\n",
        "    return label"
      ],
      "metadata": {
        "id": "99dfkbsSjLTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the audio files and labels\n",
        "train_audio, train_labels = [], []\n",
        "valid_audio, valid_labels = [], []\n",
        "# ... load and split train/validation audio and labels ...\n",
        "\n",
        "# Define the function to preprocess the audio\n",
        "def preprocess_audio(audio):\n",
        "    # Convert the audio to float32 and reshape to a 2D tensor\n",
        "    audio = tf.reshape(tf.cast(audio, tf.float32), [1, -1])\n",
        "\n",
        "    # Perform STFT with absolute value and raise to the power of 0.5\n",
        "    stft = tf.abs(tf.signal.stft(audio, frame_length=512, frame_step=160, fft_length=512))\n",
        "    spectrogram = tf.pow(stft, 0.5)\n",
        "\n",
        "    # Compute the mean and standard deviation and normalize the spectrogram\n",
        "    mean = tf.reduce_mean(spectrogram)\n",
        "    stddevs = tf.math.reduce_std(spectrogram)\n",
        "    spectrogram = (spectrogram - mean) / (stddevs + 1e-10)\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "# Define the function to preprocess the labels\n",
        "def preprocess_label(label):\n",
        "    # Convert the label to lowercase and split into Unicode characters\n",
        "    label = tf.strings.lower(label)\n",
        "    label = tf.strings.unicode_split(label, 'UTF-8')\n",
        "    return label\n",
        "\n",
        "# Create the train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_audio, train_labels))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_labels))\n",
        "train_dataset = train_dataset.map(lambda audio, label: (preprocess_audio(audio), preprocess_label(label)))\n",
        "train_dataset = train_dataset.padded_batch(batch_size=32, padded_shapes=([None, None], [None]))\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Create the validation dataset\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_audio, valid_labels))\n",
        "valid_dataset = valid_dataset.map(lambda audio, label: (preprocess_audio(audio), preprocess_label(label)))\n",
        "valid_dataset = valid_dataset.padded_batch(batch_size=32, padded_shapes=([None, None], [None]))\n",
        "valid_dataset = valid_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "gNi6SNe4iJR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Define the function to plot the audio signal, spectrogram, and transcription\n",
        "def plot_audio(audio, label):\n",
        "    # Preprocess the audio and label\n",
        "    spectrogram = preprocess_audio(audio)\n",
        "    label = preprocess_label(label)\n",
        "\n",
        "    # Plot the audio signal\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(audio)\n",
        "    plt.title('Audio Signal')\n",
        "    plt.xlabel('Time (samples)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the spectrogram\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(spectrogram.T, origin='lower', cmap='gray', aspect='auto')\n",
        "    plt.title('Spectrogram')\n",
        "    plt.xlabel('Time (frames)')\n",
        "    plt.ylabel('Frequency (bins)')\n",
        "    plt.show()\n",
        "\n",
        "    # Print the transcription\n",
        "    transcription = ''.join(tf.strings.unicode_decode(label, 'UTF-8').numpy())\n",
        "    print('Transcription:', transcription)\n",
        "\n",
        "# Visualize the first element of the train dataset\n",
        "plot_audio(train_audio[0], train_labels[0])"
      ],
      "metadata": {
        "id": "lA42y6Dcj6P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    input_spectrogram = layers.Input(shape=(None, input_dim))\n",
        "\n",
        "    # Reshape the input to a 4D tensor\n",
        "    x = layers.Reshape((-1, input_dim, 1))(input_spectrogram)\n",
        "\n",
        "    # Apply 2D convolutional layers to learn features from the input\n",
        "    x = layers.Conv2D(filters=32, kernel_size=(11, 41), strides=[2,2],activation='relu', padding='same',use_bias= False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "    x = layers.Conv2D(filters=32, kernel_size=(11, 21), strides=[1,2],activation='relu', padding='same',use_bias= False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Reshape((-1, x.shape[-2]*x.shape[-1]))(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Apply recurrent layers to learn temporal dependencies in the input\n",
        "    for i in range(rnn_layers):\n",
        "    \trecurrent = layers.GRU(\n",
        "    \t\tunits = rnn_units,\n",
        "    \t\tactivation = 'tanh',\n",
        "    \t\trecurrent_activation = 'sigmoid',\n",
        "    \t\tuse_bias = True,\n",
        "    \t\treturn_sequences = True,\n",
        "    \t\treset_after = True\n",
        "\n",
        "    \t)\n",
        "\n",
        "\n",
        "    x = layers.Bidirectional(recurrent ,merge_mode='concat')(x)\n",
        "    if i < rnn_layers:\n",
        "      x= layers.Dropout(rate=0.5)(x)\n",
        "\n",
        "    # Map the output to a prediction over characters'\n",
        "    x = layers.Dense(units=rnn_units*2, activation='relu')(x)\n",
        "    x= layers.Dropout(rate=0.5)(x)\n",
        "\n",
        "\n",
        "\n",
        "    #+1 is for the blank symbol used in the CTC (Connectionist Temporal Classification) loss function.\n",
        "    x = layers.Dense(units=output_dim+1, activation='softmax')(x)\n",
        "\n",
        "    # Define the input and output of the model\n",
        "    model = tf.keras.models.Model(inputs=input_spectrogram, outputs=x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "NXPRs6GaUKM5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i9HztqcWV7XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(input_dim = fft_length // 2 + 1 , output_dim= char_to_num.vocabulary_size() , rnn_units = 512)\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q9zNVdsUg-P",
        "outputId": "1001066b-2efe-4a9d-cb75-9a58433f74d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None, 193)]       0         \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, None, 193, 1)      0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, None, 97, 32)      14432     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, None, 97, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, None, 49, 32)      236544    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, None, 49, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, None, 1568)        0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, None, 1024)       6395904   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, None, 1024)        0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, None, 1024)        1049600   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, None, 1024)        0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, None, 32)          32800     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,729,536\n",
            "Trainable params: 7,729,408\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the model architecture\n",
        "plot_model(model, to_file='deepspeech2.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "xdErTSZGW20x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_batch_predictions(pred):\n",
        "\tinput_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "\t# Use Greedy Search , For complex task , you can use beas=m search\n",
        "\tresults = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "\toutput_text = []\n",
        "\tfor result in results:\n",
        "\t\tresult = tf.strings.reduce_join(num_to_char(result)).numpy().decode('utf-8')\n",
        "\t\toutput_text.append(result)\n",
        "\treturn output_text\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "BjD-TnZZaiXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "LdoOxeDBcAlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "targets = []\n",
        "\n",
        "for batch in validation_dataset:\n",
        "\tX,y = batch\n",
        "\tbatch_predictions = model.predict(X)\n",
        "\tbatch_predictions = decode_batch_predictions(batch_predictions)\n",
        "\tpredictions.extend(batch_predictions)\n",
        "\tfor label in y:\n",
        "\t\tlabel = tf.strings.reduce_join(num_to_char(label)).numpy().decode('utf-8')\n",
        "\t\ttargets.append(label)\n",
        "\n",
        "wer_score = wer(targets, predictions)\n",
        "print(f\"WER : {wer_score}\")"
      ],
      "metadata": {
        "id": "Wm3hS-6PcEV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}